{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui9at2wpW5gI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "from scipy import stats\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Intro to AI-520/Better Smarter Faster/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8J8uIjU0Yk-",
        "outputId": "3eaaef4a-11bf-4b5a-d5b0-2c558febd4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Colab Notebooks/Intro to AI-520/Better Smarter Faster\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Our Data/u_partial.pickle', 'rb') as f:\n",
        "  utility = pickle.load(f)"
      ],
      "metadata": {
        "id": "sGlmmJsb0Me1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create data\n",
        "\n",
        "size = 50\n",
        "reshape = []\n",
        "for i in range(1, size+1):\n",
        "  for j in range(1, size+1):\n",
        "    belief_state = utility[(i-1,j-1)][0]\n",
        "      \n",
        "    preyProb = np.array(belief_state)\n",
        "    MaxPreyPos = float(np.argmax(preyProb))\n",
        "    PreyProbMin = np.min(preyProb)\n",
        "    PreyProbMax = np.max(preyProb)\n",
        "    PreyProbMean = np.mean(preyProb)\n",
        "    PreyProbMode = stats.mode(preyProb)[0]\n",
        "\n",
        "    reshape.append([float(i), float(j), MaxPreyPos, PreyProbMin, PreyProbMax, PreyProbMean, PreyProbMode, float(utility[i-1][j-1])])\n",
        "  "
      ],
      "metadata": {
        "id": "DJwLF6j-cvDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Just for Visualization of Features of the Data\n",
        "data = pd.DataFrame(reshape, columns = ['Agent','Predator', 'MaxPreyPos', 'PreyProbMin', 'PreyProbMax', 'PreyProbMean', 'PreyProbMode', 'Utility'])\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FIKe6TsgVlmx",
        "outputId": "6d74f9e1-6fa7-40f5-d9cc-6b28d6954a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Agent  Predator  MaxPreyPos  PreyProbMin  PreyProbMax  PreyProbMean  \\\n",
              "0    1.0       1.0         4.0          0.0     0.933721          0.02   \n",
              "1    1.0       2.0        13.0          0.0     0.814050          0.02   \n",
              "2    1.0       3.0         2.0          0.0     0.927228          0.02   \n",
              "3    1.0       4.0        28.0          0.0     0.583193          0.02   \n",
              "4    1.0       5.0        37.0          0.0     0.803965          0.02   \n",
              "\n",
              "  PreyProbMode  Utility  \n",
              "0        [0.0]     17.0  \n",
              "1        [0.0]     17.0  \n",
              "2        [0.0]     16.0  \n",
              "3        [0.0]     25.0  \n",
              "4        [0.0]     12.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a4c1ba7-1110-4a8d-8a35-3bb80ab31b7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Agent</th>\n",
              "      <th>Predator</th>\n",
              "      <th>MaxPreyPos</th>\n",
              "      <th>PreyProbMin</th>\n",
              "      <th>PreyProbMax</th>\n",
              "      <th>PreyProbMean</th>\n",
              "      <th>PreyProbMode</th>\n",
              "      <th>Utility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.933721</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.814050</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.927228</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.583193</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.803965</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a4c1ba7-1110-4a8d-8a35-3bb80ab31b7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a4c1ba7-1110-4a8d-8a35-3bb80ab31b7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a4c1ba7-1110-4a8d-8a35-3bb80ab31b7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec = np.array(reshape)\n",
        "m,n = vec.shape\n",
        "np.random.shuffle(vec)\n",
        "print(m,n)\n",
        "\n",
        "# training_split = math.ceil(m*0.8)\n",
        "#Training split\n",
        "training = vec[0:2000].T\n",
        "x_train = training[0:n-1].T\n",
        "y_train = training[n-1].T\n",
        "\n",
        "#Testing split\n",
        "testing = vec[2001: m]\n",
        "x_test = testing[0:n-1]\n",
        "y_test = testing[n-1]"
      ],
      "metadata": {
        "id": "vJ8hJRT0eIpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ac76fe-90ac-4c6c-dc24-ae31b1521a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-5b9707695c87>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  vec = np.array(reshape)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(x_train)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "na2QzYGwofba",
        "outputId": "9db40ddd-2180-4425-b2d5-828300e7df38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0     1     2    3         4     5      6\n",
              "0  16.0  38.0  23.0  0.0  0.590572  0.02  [0.0]\n",
              "1  42.0  35.0   0.0  0.0  0.692585  0.02  [0.0]\n",
              "2  41.0  27.0   4.0  0.0  0.932666  0.02  [0.0]\n",
              "3  25.0   6.0  32.0  0.0  0.419042  0.02  [0.0]\n",
              "4  15.0  35.0  36.0  0.0  0.471157  0.02  [0.0]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9955b9a4-4d99-4455-98a1-1c0828256f91\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.590572</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.692585</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.932666</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.419042</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.471157</td>\n",
              "      <td>0.02</td>\n",
              "      <td>[0.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9955b9a4-4d99-4455-98a1-1c0828256f91')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9955b9a4-4d99-4455-98a1-1c0828256f91 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9955b9a4-4d99-4455-98a1-1c0828256f91');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(y_train)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eSqqP4IEkg24",
        "outputId": "fea62fd0-f6f9-43d7-c495-20c01ea14d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0\n",
              "0  24.0\n",
              "1  27.0\n",
              "2  28.0\n",
              "3  21.0\n",
              "4  12.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77802d4c-61cf-41ec-839a-83b1ab692e1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77802d4c-61cf-41ec-839a-83b1ab692e1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77802d4c-61cf-41ec-839a-83b1ab692e1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77802d4c-61cf-41ec-839a-83b1ab692e1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples , features = x_train.shape\n",
        "examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWxP4bOWkuFF",
        "outputId": "99e719fc-3f4d-4e2f-e064-b93983bbcdfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ForwardPass:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.weights = np.random.randn(input_size, output_size) / np.sqrt(100*(input_size + output_size))\n",
        "    self.bias = np.random.randn(1, output_size) / np.sqrt(100*(input_size + output_size))\n",
        "\n",
        "  def forwardProp(self, input):\n",
        "    self.input = input\n",
        "    return np.dot(input, self.weights) + self.bias\n",
        "\n",
        "  def backwardProp(self, output_error, learning_rate):\n",
        "    input_error = np.dot(output_error, self.weights.T)\n",
        "    weights_error = np.dot(self.input.T, output_error)\n",
        "    # bias_error = output_error\n",
        "    \n",
        "    self.weights -= learning_rate * weights_error\n",
        "    self.bias -= learning_rate * output_error\n",
        "    return input_error"
      ],
      "metadata": {
        "id": "_dAh4j5kAaeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationFunction:\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "    \n",
        "    def forwardProp(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(input)\n",
        "    \n",
        "    def backwardProp(self, output_error, learning_rate):\n",
        "        return output_error * self.activation_prime(self.input)"
      ],
      "metadata": {
        "id": "twbtAMG1Ax6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_ReLU(x):\n",
        "  #vanilla relu\n",
        "  # return np.maximum(0, Z)\n",
        "  return np.where(x > 0, x, x * 0.01)  \n",
        "\n",
        "def leaky_ReLU_prime(x):\n",
        "  #vanilla relu\n",
        "  # return Z > 0\n",
        "  return np.where(x > 0, 1, 0.01)  "
      ],
      "metadata": {
        "id": "pofuyi6eA_m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y_true, y_pred):\n",
        "  return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "  return 2 * (y_pred - y_true) / y_pred.size"
      ],
      "metadata": {
        "id": "9g_64IFaBRLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 hidden layers neural network\n",
        "neural_network = [\n",
        "  ForwardPass(7, 20),\n",
        "  ActivationFunction(leaky_ReLU, leaky_ReLU_prime),\n",
        "  ForwardPass(20, 20),\n",
        "  ActivationFunction(leaky_ReLU, leaky_ReLU_prime),\n",
        "  ForwardPass(20, 20),\n",
        "  ActivationFunction(leaky_ReLU, leaky_ReLU_prime),\n",
        "  ForwardPass(20, 10),\n",
        "  ActivationFunction(leaky_ReLU, leaky_ReLU_prime),\n",
        "  ForwardPass(10, 1),\n",
        "  ActivationFunction(leaky_ReLU, leaky_ReLU_prime)\n",
        "]\n",
        "errors = []\n",
        "\n",
        "epochs = 500\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  error = 0\n",
        "  for x, y_true in zip(x_train, y_train):\n",
        "    # forward\n",
        "    x = x.reshape((1, x.shape[0]))\n",
        "    output = x\n",
        "    for layer in neural_network:\n",
        "      output = layer.forwardProp(output)\n",
        "\n",
        "    error += mse(y_true, output)\n",
        "\n",
        "    # backward\n",
        "    output_error = mse_prime(y_true, output)\n",
        "    for layer in reversed(neural_network):\n",
        "      output_error = layer.backwardProp(output_error, learning_rate)\n",
        "  \n",
        "  error /= len(x_train)\n",
        "  print('%d/%d, error=%f' % (epoch + 1, epochs, error))\n",
        "  errors.append(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zUN45DvBXUo",
        "outputId": "d41edd84-41e5-48b8-ff39-cb048ddf7a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for iteration 0 => 578.099\n",
            "Loss for iteration 1 => 572.3180100000001\n",
            "Loss for iteration 2 => 566.5948299\n",
            "Loss for iteration 3 => 560.9288816010001\n",
            "Loss for iteration 4 => 555.31959278499\n",
            "Loss for iteration 5 => 549.7663968571402\n",
            "Loss for iteration 6 => 544.2687328885688\n",
            "Loss for iteration 7 => 538.8260455596832\n",
            "Loss for iteration 8 => 533.4377851040863\n",
            "Loss for iteration 9 => 528.1034072530455\n",
            "Loss for iteration 10 => 522.822373180515\n",
            "Loss for iteration 11 => 517.5941494487098\n",
            "Loss for iteration 12 => 512.4182079542227\n",
            "Loss for iteration 13 => 507.2940258746805\n",
            "Loss for iteration 14 => 502.22108561593365\n",
            "Loss for iteration 15 => 497.1988747597743\n",
            "Loss for iteration 16 => 492.2268860121766\n",
            "Loss for iteration 17 => 487.3046171520548\n",
            "Loss for iteration 18 => 482.4315709805342\n",
            "Loss for iteration 19 => 477.6072552707289\n",
            "Loss for iteration 20 => 472.83118271802164\n",
            "Loss for iteration 21 => 468.1028708908414\n",
            "Loss for iteration 22 => 463.421842181933\n",
            "Loss for iteration 23 => 458.78762376011366\n",
            "Loss for iteration 24 => 454.19974752251255\n",
            "Loss for iteration 25 => 449.65775004728744\n",
            "Loss for iteration 26 => 445.16117254681456\n",
            "Loss for iteration 27 => 440.70956082134643\n",
            "Loss for iteration 28 => 436.30246521313296\n",
            "Loss for iteration 29 => 431.9394405610016\n",
            "Loss for iteration 30 => 427.6200461553916\n",
            "Loss for iteration 31 => 423.3438456938377\n",
            "Loss for iteration 32 => 419.11040723689933\n",
            "Loss for iteration 33 => 414.91930316453033\n",
            "Loss for iteration 34 => 410.770110132885\n",
            "Loss for iteration 35 => 406.66240903155614\n",
            "Loss for iteration 36 => 402.5957849412406\n",
            "Loss for iteration 37 => 398.5698270918282\n",
            "Loss for iteration 38 => 394.5841288209099\n",
            "Loss for iteration 39 => 390.63828753270076\n",
            "Loss for iteration 40 => 386.7319046573738\n",
            "Loss for iteration 41 => 382.8645856108\n",
            "Loss for iteration 42 => 379.03593975469204\n",
            "Loss for iteration 43 => 375.2455803571451\n",
            "Loss for iteration 44 => 371.49312455357364\n",
            "Loss for iteration 45 => 367.7781933080379\n",
            "Loss for iteration 46 => 364.1004113749575\n",
            "Loss for iteration 47 => 360.4594072612079\n",
            "Loss for iteration 48 => 356.8548131885958\n",
            "Loss for iteration 49 => 353.28626505670985\n",
            "Loss for iteration 50 => 349.75340240614275\n",
            "Loss for iteration 51 => 346.2558683820813\n",
            "Loss for iteration 52 => 342.7933096982605\n",
            "Loss for iteration 53 => 339.3653766012779\n",
            "Loss for iteration 54 => 335.9717228352651\n",
            "Loss for iteration 55 => 332.6120056069124\n",
            "Loss for iteration 56 => 329.2858855508433\n",
            "Loss for iteration 57 => 325.9930266953349\n",
            "Loss for iteration 58 => 322.73309642838154\n",
            "Loss for iteration 59 => 319.50576546409775\n",
            "Loss for iteration 60 => 316.31070780945674\n",
            "Loss for iteration 61 => 313.14760073136216\n",
            "Loss for iteration 62 => 310.01612472404855\n",
            "Loss for iteration 63 => 306.9159634768081\n",
            "Loss for iteration 64 => 303.84680384204\n",
            "Loss for iteration 65 => 300.8083358036196\n",
            "Loss for iteration 66 => 297.80025244558345\n",
            "Loss for iteration 67 => 294.8222499211276\n",
            "Loss for iteration 68 => 291.8740274219163\n",
            "Loss for iteration 69 => 288.95528714769716\n",
            "Loss for iteration 70 => 286.0657342762202\n",
            "Loss for iteration 71 => 283.20507693345803\n",
            "Loss for iteration 72 => 280.3730261641235\n",
            "Loss for iteration 73 => 277.5692959024822\n",
            "Loss for iteration 74 => 274.7936029434574\n",
            "Loss for iteration 75 => 272.04566691402283\n",
            "Loss for iteration 76 => 269.3252102448826\n",
            "Loss for iteration 77 => 266.6319581424338\n",
            "Loss for iteration 78 => 263.9656385610095\n",
            "Loss for iteration 79 => 261.3259821753994\n",
            "Loss for iteration 80 => 258.7127223536454\n",
            "Loss for iteration 81 => 256.12559513010893\n",
            "Loss for iteration 82 => 253.56433917880784\n",
            "Loss for iteration 83 => 251.02869578701976\n",
            "Loss for iteration 84 => 248.51840882914956\n",
            "Loss for iteration 85 => 246.03322474085806\n",
            "Loss for iteration 86 => 243.57289249344947\n",
            "Loss for iteration 87 => 241.13716356851498\n",
            "Loss for iteration 88 => 238.72579193282982\n",
            "Loss for iteration 89 => 236.33853401350152\n",
            "Loss for iteration 90 => 233.9751486733665\n",
            "Loss for iteration 91 => 231.63539718663284\n",
            "Loss for iteration 92 => 229.3190432147665\n",
            "Loss for iteration 93 => 227.02585278261884\n",
            "Loss for iteration 94 => 224.75559425479264\n",
            "Loss for iteration 95 => 222.5080383122447\n",
            "Loss for iteration 96 => 220.28295792912226\n",
            "Loss for iteration 97 => 218.08012834983103\n",
            "Loss for iteration 98 => 215.89932706633272\n",
            "Loss for iteration 99 => 213.7403337956694\n",
            "Loss for iteration 100 => 211.6029304577127\n",
            "Loss for iteration 101 => 209.48690115313556\n",
            "Loss for iteration 102 => 207.3920321416042\n",
            "Loss for iteration 103 => 205.31811182018816\n",
            "Loss for iteration 104 => 203.2649307019863\n",
            "Loss for iteration 105 => 201.23228139496644\n",
            "Loss for iteration 106 => 199.21995858101678\n",
            "Loss for iteration 107 => 197.2277589952066\n",
            "Loss for iteration 108 => 195.25548140525456\n",
            "Loss for iteration 109 => 193.302926591202\n",
            "Loss for iteration 110 => 191.36989732529\n",
            "Loss for iteration 111 => 189.4561983520371\n",
            "Loss for iteration 112 => 187.56163636851673\n",
            "Loss for iteration 113 => 185.68602000483156\n",
            "Loss for iteration 114 => 183.82915980478325\n",
            "Loss for iteration 115 => 181.9908682067354\n",
            "Loss for iteration 116 => 180.17095952466804\n",
            "Loss for iteration 117 => 178.36924992942136\n",
            "Loss for iteration 118 => 176.58555743012715\n",
            "Loss for iteration 119 => 174.81970185582588\n",
            "Loss for iteration 120 => 173.0715048372676\n",
            "Loss for iteration 121 => 171.34078978889494\n",
            "Loss for iteration 122 => 169.627381891006\n",
            "Loss for iteration 123 => 167.93110807209592\n",
            "Loss for iteration 124 => 166.25179699137496\n",
            "Loss for iteration 125 => 164.5892790214612\n",
            "Loss for iteration 126 => 162.9433862312466\n",
            "Loss for iteration 127 => 161.31395236893414\n",
            "Loss for iteration 128 => 159.7008128452448\n",
            "Loss for iteration 129 => 158.10380471679235\n",
            "Loss for iteration 130 => 156.52276666962442\n",
            "Loss for iteration 131 => 154.95753900292817\n",
            "Loss for iteration 132 => 153.4079636128989\n",
            "Loss for iteration 133 => 151.87388397676992\n",
            "Loss for iteration 134 => 150.3551451370022\n",
            "Loss for iteration 135 => 148.8515936856322\n",
            "Loss for iteration 136 => 147.36307774877588\n",
            "Loss for iteration 137 => 145.88944697128812\n",
            "Loss for iteration 138 => 144.43055250157525\n",
            "Loss for iteration 139 => 142.9862469765595\n",
            "Loss for iteration 140 => 141.5563845067939\n",
            "Loss for iteration 141 => 140.14082066172597\n",
            "Loss for iteration 142 => 138.73941245510872\n",
            "Loss for iteration 143 => 137.35201833055763\n",
            "Loss for iteration 144 => 135.97849814725205\n",
            "Loss for iteration 145 => 134.61871316577952\n",
            "Loss for iteration 146 => 133.27252603412174\n",
            "Loss for iteration 147 => 131.93980077378052\n",
            "Loss for iteration 148 => 130.6204027660427\n",
            "Loss for iteration 149 => 129.3141987383823\n",
            "Loss for iteration 150 => 128.02105675099847\n",
            "Loss for iteration 151 => 126.74084618348849\n",
            "Loss for iteration 152 => 125.4734377216536\n",
            "Loss for iteration 153 => 124.21870334443706\n",
            "Loss for iteration 154 => 122.9765163109927\n",
            "Loss for iteration 155 => 121.74675114788276\n",
            "Loss for iteration 156 => 120.52928363640393\n",
            "Loss for iteration 157 => 119.3239908000399\n",
            "Loss for iteration 158 => 118.1307508920395\n",
            "Loss for iteration 159 => 116.9494433831191\n",
            "Loss for iteration 160 => 115.77994894928791\n",
            "Loss for iteration 161 => 114.62214945979504\n",
            "Loss for iteration 162 => 113.47592796519709\n",
            "Loss for iteration 163 => 112.34116868554511\n",
            "Loss for iteration 164 => 111.21775699868967\n",
            "Loss for iteration 165 => 110.10557942870277\n",
            "Loss for iteration 166 => 109.00452363441575\n",
            "Loss for iteration 167 => 107.9144783980716\n",
            "Loss for iteration 168 => 106.83533361409089\n",
            "Loss for iteration 169 => 105.76698027794998\n",
            "Loss for iteration 170 => 104.70931047517048\n",
            "Loss for iteration 171 => 103.66221737041877\n",
            "Loss for iteration 172 => 102.62559519671458\n",
            "Loss for iteration 173 => 101.59933924474744\n",
            "Loss for iteration 174 => 100.58334585229997\n",
            "Loss for iteration 175 => 99.57751239377697\n",
            "Loss for iteration 176 => 98.5817372698392\n",
            "Loss for iteration 177 => 97.5959198971408\n",
            "Loss for iteration 178 => 96.6199606981694\n",
            "Loss for iteration 179 => 95.6537610911877\n",
            "Loss for iteration 180 => 94.69722348027582\n",
            "Loss for iteration 181 => 93.75025124547307\n",
            "Loss for iteration 182 => 92.81274873301834\n",
            "Loss for iteration 183 => 91.88462124568815\n",
            "Loss for iteration 184 => 90.96577503323127\n",
            "Loss for iteration 185 => 90.05611728289895\n",
            "Loss for iteration 186 => 89.15555611006997\n",
            "Loss for iteration 187 => 88.26400054896926\n",
            "Loss for iteration 188 => 87.38136054347957\n",
            "Loss for iteration 189 => 86.50754693804478\n",
            "Loss for iteration 190 => 85.64247146866434\n",
            "Loss for iteration 191 => 84.7860467539777\n",
            "Loss for iteration 192 => 83.93818628643793\n",
            "Loss for iteration 193 => 83.09880442357354\n",
            "Loss for iteration 194 => 82.2678163793378\n",
            "Loss for iteration 195 => 81.44513821554442\n",
            "Loss for iteration 196 => 80.63068683338898\n",
            "Loss for iteration 197 => 79.82437996505509\n",
            "Loss for iteration 198 => 79.02613616540454\n",
            "Loss for iteration 199 => 78.23587480375049\n",
            "Loss for iteration 200 => 77.45351605571298\n",
            "Loss for iteration 201 => 76.67898089515585\n",
            "Loss for iteration 202 => 75.91219108620429\n",
            "Loss for iteration 203 => 75.15306917534224\n",
            "Loss for iteration 204 => 74.40153848358882\n",
            "Loss for iteration 205 => 73.65752309875293\n",
            "Loss for iteration 206 => 72.9209478677654\n",
            "Loss for iteration 207 => 72.19173838908775\n",
            "Loss for iteration 208 => 71.46982100519686\n",
            "Loss for iteration 209 => 70.7551227951449\n",
            "Loss for iteration 210 => 70.04757156719344\n",
            "Loss for iteration 211 => 69.34709585152152\n",
            "Loss for iteration 212 => 68.6536248930063\n",
            "Loss for iteration 213 => 67.96708864407623\n",
            "Loss for iteration 214 => 67.28741775763547\n",
            "Loss for iteration 215 => 66.6145435800591\n",
            "Loss for iteration 216 => 65.94839814425852\n",
            "Loss for iteration 217 => 65.28891416281593\n",
            "Loss for iteration 218 => 64.63602502118776\n",
            "Loss for iteration 219 => 63.989664770975885\n",
            "Loss for iteration 220 => 63.349768123266124\n",
            "Loss for iteration 221 => 62.71627044203346\n",
            "Loss for iteration 222 => 62.089107737613126\n",
            "Loss for iteration 223 => 61.468216660237\n",
            "Loss for iteration 224 => 60.853534493634626\n",
            "Loss for iteration 225 => 60.24499914869828\n",
            "Loss for iteration 226 => 59.64254915721129\n",
            "Loss for iteration 227 => 59.04612366563918\n",
            "Loss for iteration 228 => 58.455662428982784\n",
            "Loss for iteration 229 => 57.87110580469296\n",
            "Loss for iteration 230 => 57.29239474664603\n",
            "Loss for iteration 231 => 56.71947079917957\n",
            "Loss for iteration 232 => 56.15227609118777\n",
            "Loss for iteration 233 => 55.590753330275895\n",
            "Loss for iteration 234 => 55.03484579697314\n",
            "Loss for iteration 235 => 54.484497339003404\n",
            "Loss for iteration 236 => 53.93965236561337\n",
            "Loss for iteration 237 => 53.40025584195723\n",
            "Loss for iteration 238 => 52.86625328353766\n",
            "Loss for iteration 239 => 52.33759075070228\n",
            "Loss for iteration 240 => 51.81421484319526\n",
            "Loss for iteration 241 => 51.2960726947633\n",
            "Loss for iteration 242 => 50.78311196781567\n",
            "Loss for iteration 243 => 50.27528084813751\n",
            "Loss for iteration 244 => 49.77252803965614\n",
            "Loss for iteration 245 => 49.27480275925958\n",
            "Loss for iteration 246 => 48.78205473166698\n",
            "Loss for iteration 247 => 48.29423418435031\n",
            "Loss for iteration 248 => 47.811291842506805\n",
            "Loss for iteration 249 => 47.333178924081736\n",
            "Loss for iteration 250 => 46.85984713484092\n",
            "Loss for iteration 251 => 46.39124866349251\n",
            "Loss for iteration 252 => 45.92733617685759\n",
            "Loss for iteration 253 => 45.46806281508901\n",
            "Loss for iteration 254 => 45.013382186938124\n",
            "Loss for iteration 255 => 44.563248365068745\n",
            "Loss for iteration 256 => 44.117615881418054\n",
            "Loss for iteration 257 => 43.676439722603874\n",
            "Loss for iteration 258 => 43.23967532537784\n",
            "Loss for iteration 259 => 42.80727857212406\n",
            "Loss for iteration 260 => 42.37920578640282\n",
            "Loss for iteration 261 => 41.955413728538794\n",
            "Loss for iteration 262 => 41.535859591253406\n",
            "Loss for iteration 263 => 41.12050099534087\n",
            "Loss for iteration 264 => 40.70929598538746\n",
            "Loss for iteration 265 => 40.302203025533586\n",
            "Loss for iteration 266 => 39.89918099527825\n",
            "Loss for iteration 267 => 39.50018918532547\n",
            "Loss for iteration 268 => 39.10518729347221\n",
            "Loss for iteration 269 => 38.71413542053749\n",
            "Loss for iteration 270 => 38.32699406633211\n",
            "Loss for iteration 271 => 37.943724125668794\n",
            "Loss for iteration 272 => 37.564286884412105\n",
            "Loss for iteration 273 => 37.18864401556799\n",
            "Loss for iteration 274 => 36.81675757541231\n",
            "Loss for iteration 275 => 36.44858999965819\n",
            "Loss for iteration 276 => 36.08410409966161\n",
            "Loss for iteration 277 => 35.72326305866499\n",
            "Loss for iteration 278 => 35.36603042807834\n",
            "Loss for iteration 279 => 35.01237012379755\n",
            "Loss for iteration 280 => 34.662246422559576\n",
            "Loss for iteration 281 => 34.31562395833398\n",
            "Loss for iteration 282 => 33.97246771875064\n",
            "Loss for iteration 283 => 33.63274304156314\n",
            "Loss for iteration 284 => 33.29641561114751\n",
            "Loss for iteration 285 => 32.96345145503604\n",
            "Loss for iteration 286 => 32.63381694048567\n",
            "Loss for iteration 287 => 32.30747877108082\n",
            "Loss for iteration 288 => 31.984403983370008\n",
            "Loss for iteration 289 => 31.664559943536307\n",
            "Loss for iteration 290 => 31.347914344100943\n",
            "Loss for iteration 291 => 31.03443520065993\n",
            "Loss for iteration 292 => 30.724090848653333\n",
            "Loss for iteration 293 => 30.4168499401668\n",
            "Loss for iteration 294 => 30.11268144076513\n",
            "Loss for iteration 295 => 29.811554626357477\n",
            "Loss for iteration 296 => 29.513439080093903\n",
            "Loss for iteration 297 => 29.218304689292964\n",
            "Loss for iteration 298 => 28.926121642400034\n",
            "Loss for iteration 299 => 28.63686042597603\n",
            "Loss for iteration 300 => 28.350491821716272\n",
            "Loss for iteration 301 => 28.06698690349911\n",
            "Loss for iteration 302 => 27.78631703446412\n",
            "Loss for iteration 303 => 27.508453864119478\n",
            "Loss for iteration 304 => 27.233369325478282\n",
            "Loss for iteration 305 => 26.9610356322235\n",
            "Loss for iteration 306 => 26.691425275901263\n",
            "Loss for iteration 307 => 26.42451102314225\n",
            "Loss for iteration 308 => 26.16026591291083\n",
            "Loss for iteration 309 => 25.898663253781724\n",
            "Loss for iteration 310 => 25.639676621243908\n",
            "Loss for iteration 311 => 25.383279855031468\n",
            "Loss for iteration 312 => 25.129447056481155\n",
            "Loss for iteration 313 => 24.878152585916343\n",
            "Loss for iteration 314 => 24.62937106005718\n",
            "Loss for iteration 315 => 24.38307734945661\n",
            "Loss for iteration 316 => 24.139246575962044\n",
            "Loss for iteration 317 => 23.897854110202424\n",
            "Loss for iteration 318 => 23.6588755691004\n",
            "Loss for iteration 319 => 23.422286813409396\n",
            "Loss for iteration 320 => 23.188063945275303\n",
            "Loss for iteration 321 => 22.95618330582255\n",
            "Loss for iteration 322 => 22.72662147276432\n",
            "Loss for iteration 323 => 22.49935525803668\n",
            "Loss for iteration 324 => 22.274361705456315\n",
            "Loss for iteration 325 => 22.051618088401753\n",
            "Loss for iteration 326 => 21.831101907517734\n",
            "Loss for iteration 327 => 21.612790888442557\n",
            "Loss for iteration 328 => 21.396662979558133\n",
            "Loss for iteration 329 => 21.18269634976255\n",
            "Loss for iteration 330 => 20.970869386264926\n",
            "Loss for iteration 331 => 20.761160692402278\n",
            "Loss for iteration 332 => 20.553549085478256\n",
            "Loss for iteration 333 => 20.348013594623474\n",
            "Loss for iteration 334 => 20.144533458677238\n",
            "Loss for iteration 335 => 19.943088124090465\n",
            "Loss for iteration 336 => 19.74365724284956\n",
            "Loss for iteration 337 => 19.546220670421064\n",
            "Loss for iteration 338 => 19.350758463716854\n",
            "Loss for iteration 339 => 19.157250879079687\n",
            "Loss for iteration 340 => 18.96567837028889\n",
            "Loss for iteration 341 => 18.776021586586\n",
            "Loss for iteration 342 => 18.58826137072014\n",
            "Loss for iteration 343 => 18.40237875701294\n",
            "Loss for iteration 344 => 18.21835496944281\n",
            "Loss for iteration 345 => 18.03617141974838\n",
            "Loss for iteration 346 => 17.855809705550897\n",
            "Loss for iteration 347 => 17.67725160849539\n",
            "Loss for iteration 348 => 17.500479092410437\n",
            "Loss for iteration 349 => 17.32547430148633\n",
            "Loss for iteration 350 => 17.15221955847147\n",
            "Loss for iteration 351 => 16.980697362886755\n",
            "Loss for iteration 352 => 16.81089038925789\n",
            "Loss for iteration 353 => 16.642781485365308\n",
            "Loss for iteration 354 => 16.476353670511656\n",
            "Loss for iteration 355 => 16.31159013380654\n",
            "Loss for iteration 356 => 16.148474232468473\n",
            "Loss for iteration 357 => 15.986989490143788\n",
            "Loss for iteration 358 => 15.827119595242351\n",
            "Loss for iteration 359 => 15.668848399289928\n",
            "Loss for iteration 360 => 15.512159915297028\n",
            "Loss for iteration 361 => 15.357038316144058\n",
            "Loss for iteration 362 => 15.203467932982617\n",
            "Loss for iteration 363 => 15.05143325365279\n",
            "Loss for iteration 364 => 14.900918921116263\n",
            "Loss for iteration 365 => 14.751909731905101\n",
            "Loss for iteration 366 => 14.60439063458605\n",
            "Loss for iteration 367 => 14.458346728240189\n",
            "Loss for iteration 368 => 14.313763260957787\n",
            "Loss for iteration 369 => 14.170625628348208\n",
            "Loss for iteration 370 => 14.028919372064726\n",
            "Loss for iteration 371 => 13.888630178344078\n",
            "Loss for iteration 372 => 13.749743876560638\n",
            "Loss for iteration 373 => 13.612246437795031\n",
            "Loss for iteration 374 => 13.476123973417081\n",
            "Loss for iteration 375 => 13.34136273368291\n",
            "Loss for iteration 376 => 13.207949106346081\n",
            "Loss for iteration 377 => 13.07586961528262\n",
            "Loss for iteration 378 => 12.945110919129794\n",
            "Loss for iteration 379 => 12.815659809938497\n",
            "Loss for iteration 380 => 12.687503211839111\n",
            "Loss for iteration 381 => 12.56062817972072\n",
            "Loss for iteration 382 => 12.435021897923512\n",
            "Loss for iteration 383 => 12.310671678944278\n",
            "Loss for iteration 384 => 12.187564962154836\n",
            "Loss for iteration 385 => 12.065689312533287\n",
            "Loss for iteration 386 => 11.945032419407955\n",
            "Loss for iteration 387 => 11.825582095213875\n",
            "Loss for iteration 388 => 11.707326274261735\n",
            "Loss for iteration 389 => 11.590253011519119\n",
            "Loss for iteration 390 => 11.474350481403928\n",
            "Loss for iteration 391 => 11.359606976589887\n",
            "Loss for iteration 392 => 11.246010906823988\n",
            "Loss for iteration 393 => 11.133550797755749\n",
            "Loss for iteration 394 => 11.022215289778192\n",
            "Loss for iteration 395 => 10.91199313688041\n",
            "Loss for iteration 396 => 10.802873205511606\n",
            "Loss for iteration 397 => 10.69484447345649\n",
            "Loss for iteration 398 => 10.587896028721925\n",
            "Loss for iteration 399 => 10.482017068434706\n",
            "Loss for iteration 400 => 10.377196897750359\n",
            "Loss for iteration 401 => 10.273424928772856\n",
            "Loss for iteration 402 => 10.170690679485128\n",
            "Loss for iteration 403 => 10.068983772690277\n",
            "Loss for iteration 404 => 9.968293934963375\n",
            "Loss for iteration 405 => 9.868610995613741\n",
            "Loss for iteration 406 => 9.769924885657604\n",
            "Loss for iteration 407 => 9.672225636801027\n",
            "Loss for iteration 408 => 9.575503380433016\n",
            "Loss for iteration 409 => 9.479748346628686\n",
            "Loss for iteration 410 => 9.384950863162398\n",
            "Loss for iteration 411 => 9.291101354530774\n",
            "Loss for iteration 412 => 9.198190340985466\n",
            "Loss for iteration 413 => 9.106208437575612\n",
            "Loss for iteration 414 => 9.015146353199857\n",
            "Loss for iteration 415 => 8.924994889667857\n",
            "Loss for iteration 416 => 8.835744940771178\n",
            "Loss for iteration 417 => 8.747387491363467\n",
            "Loss for iteration 418 => 8.659913616449833\n",
            "Loss for iteration 419 => 8.573314480285335\n",
            "Loss for iteration 420 => 8.487581335482481\n",
            "Loss for iteration 421 => 8.402705522127656\n",
            "Loss for iteration 422 => 8.318678466906379\n",
            "Loss for iteration 423 => 8.235491682237315\n",
            "Loss for iteration 424 => 8.153136765414942\n",
            "Loss for iteration 425 => 8.071605397760793\n",
            "Loss for iteration 426 => 7.990889343783185\n",
            "Loss for iteration 427 => 7.910980450345353\n",
            "Loss for iteration 428 => 7.831870645841899\n",
            "Loss for iteration 429 => 7.75355193938348\n",
            "Loss for iteration 430 => 7.6760164199896455\n",
            "Loss for iteration 431 => 7.599256255789749\n",
            "Loss for iteration 432 => 7.5232636932318515\n",
            "Loss for iteration 433 => 7.448031056299533\n",
            "Loss for iteration 434 => 7.373550745736538\n",
            "Loss for iteration 435 => 7.299815238279173\n",
            "Loss for iteration 436 => 7.226817085896381\n",
            "Loss for iteration 437 => 7.1545489150374175\n",
            "Loss for iteration 438 => 7.083003425887044\n",
            "Loss for iteration 439 => 7.012173391628173\n",
            "Loss for iteration 440 => 6.942051657711891\n",
            "Loss for iteration 441 => 6.8726311411347725\n",
            "Loss for iteration 442 => 6.803904829723424\n",
            "Loss for iteration 443 => 6.73586578142619\n",
            "Loss for iteration 444 => 6.668507123611928\n",
            "Loss for iteration 445 => 6.601822052375809\n",
            "Loss for iteration 446 => 6.535803831852051\n",
            "Loss for iteration 447 => 6.47044579353353\n",
            "Loss for iteration 448 => 6.405741335598195\n",
            "Loss for iteration 449 => 6.341683922242213\n",
            "Loss for iteration 450 => 6.278267083019791\n",
            "Loss for iteration 451 => 6.215484412189593\n",
            "Loss for iteration 452 => 6.153329568067697\n",
            "Loss for iteration 453 => 6.09179627238702\n",
            "Loss for iteration 454 => 6.03087830966315\n",
            "Loss for iteration 455 => 5.970569526566519\n",
            "Loss for iteration 456 => 5.910863831300853\n",
            "Loss for iteration 457 => 5.851755192987844\n",
            "Loss for iteration 458 => 5.793237641057965\n",
            "Loss for iteration 459 => 5.735305264647386\n",
            "Loss for iteration 460 => 5.677952212000912\n",
            "Loss for iteration 461 => 5.621172689880903\n",
            "Loss for iteration 462 => 5.564960962982094\n",
            "Loss for iteration 463 => 5.509311353352273\n",
            "Loss for iteration 464 => 5.454218239818751\n",
            "Loss for iteration 465 => 5.399676057420563\n",
            "Loss for iteration 466 => 5.345679296846357\n",
            "Loss for iteration 467 => 5.292222503877893\n",
            "Loss for iteration 468 => 5.239300278839114\n",
            "Loss for iteration 469 => 5.186907276050722\n",
            "Loss for iteration 470 => 5.1350382032902155\n",
            "Loss for iteration 471 => 5.083687821257313\n",
            "Loss for iteration 472 => 5.03285094304474\n",
            "Loss for iteration 473 => 4.982522433614292\n",
            "Loss for iteration 474 => 4.93269720927815\n",
            "Loss for iteration 475 => 4.8833702371853684\n",
            "Loss for iteration 476 => 4.834536534813515\n",
            "Loss for iteration 477 => 4.78619116946538\n",
            "Loss for iteration 478 => 4.738329257770726\n",
            "Loss for iteration 479 => 4.690945965193019\n",
            "Loss for iteration 480 => 4.644036505541089\n",
            "Loss for iteration 481 => 4.5975961404856776\n",
            "Loss for iteration 482 => 4.551620179080821\n",
            "Loss for iteration 483 => 4.506103977290013\n",
            "Loss for iteration 484 => 4.461042937517113\n",
            "Loss for iteration 485 => 4.416432508141941\n",
            "Loss for iteration 486 => 4.372268183060522\n",
            "Loss for iteration 487 => 4.328545501229917\n",
            "Loss for iteration 488 => 4.285260046217618\n",
            "Loss for iteration 489 => 4.242407445755442\n",
            "Loss for iteration 490 => 4.199983371297888\n",
            "Loss for iteration 491 => 4.157983537584909\n",
            "Loss for iteration 492 => 4.11640370220906\n",
            "Loss for iteration 493 => 4.075239665186969\n",
            "Loss for iteration 494 => 4.034487268535099\n",
            "Loss for iteration 495 => 3.9941423958497486\n",
            "Loss for iteration 496 => 3.954200971891251\n",
            "Loss for iteration 497 => 3.914658962172339\n",
            "Loss for iteration 498 => 3.8755123725506153\n",
            "Loss for iteration 499 => 3.8367572488251094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.arange(len(errors))\n",
        "Y = np.array(errors)\n",
        "plt.xlabel(\"Epochs\")  \n",
        "plt.ylabel(\"Error Value\")  \n",
        "plt.plot(X,Y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "2Wde7RccizGS",
        "outputId": "e247a4a3-7d84-433b-e4f4-722303a29244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X):\n",
        "  for layer in neural_network:\n",
        "    output = layer.forwardProp(output)\n",
        "  return output"
      ],
      "metadata": {
        "id": "eCWhXVH03Jow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_partial= {}\n",
        "for i in range(2500):\n",
        "  X = data.iloc[i,:-1].values\n",
        "  X = np.expand_dims(X,axis = 1)\n",
        "  y = predict(X)\n",
        "  V_partial[(X[0][0],X[1][0],X[2][0])] = y[0][0]"
      ],
      "metadata": {
        "id": "EdLGbALU3L7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Our Data/v_partial.pickle\", \"wb\") as f:\n",
        "  pickle.dump(V_partial,f)"
      ],
      "metadata": {
        "id": "gg4kD4PC3OAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}